{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:35.314087Z",
     "start_time": "2020-08-24T15:21:33.275422Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:35.319564Z",
     "start_time": "2020-08-24T15:21:35.316076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.0\n",
      "Keras version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>The whole Data API revolves around the concept of a <em><font color='red' size=2>dataset</font></em>, which represents <i>a sequence of data items.</i></b>\n",
    "</div>\n",
    "\n",
    "整个 Data API 的核心概念是 dataset，表示 a sequence of data items。\n",
    "\n",
    "通常，通过从 disk 逐步读取数据 (gradually read data from disk) 来使用数据。\n",
    "\n",
    "for simplicity，使用 `tf.data.Dataset.from_tensor_slices()` 创建一个完全在 RAM 中的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:36.961282Z",
     "start_time": "2020-08-24T15:21:35.320899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant([1., 3., 5.])\n",
    "# Creates a Dataset whose elements are slices of the given tensors.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.185419Z",
     "start_time": "2020-08-24T15:21:36.964856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.data.Dataset.from_tensor_slices()`** 接收一个 tensor 作为参数，然后创建一个 `tf.data.Dataset`——其元素全是 `X` 的 slices（沿着第一个 axis）。因此，这个 `dataset` 含有 10 个元素：tensor 0, tensor 1, tensor 2, ..., tensor 9。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.191828Z",
     "start_time": "2020-08-24T15:21:37.187166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RangeDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以获取相同的 dataset\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.218836Z",
     "start_time": "2020-08-24T15:21:37.195712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over dataset's items\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chain transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了 dataset 之后，就可以通过调用 dataset 的 method 来完成各种类型的 transformations。\n",
    "\n",
    "**<font color='crimson'>dataset 的每一个 method 都会返回一个新的 dataset。</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.240259Z",
     "start_time": "2020-08-24T15:21:37.221454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.repeat(2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.255646Z",
     "start_time": "2020-08-24T15:21:37.241791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Chain transformations\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "    \n",
    "![chaining dataset transformations](chap13_figs/chaining_dataset_transformations.png)\n",
    "<br>\n",
    "    \n",
    "<center><i>Chain dataset transformations</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首先，在原始 `dataset` 上调用 **`repeat()`** 方法，这返回一个新的 dataset——复制原始 dataset 的元素 (items) 3 次。当然，这**不是在内存中复制 3 次 dataset**。如果**调用 `repeat()` 时没有指定参数**，那么返回的新 dataset 会无限复制原始 dataset，此时需要在代码中决定合适停止 iteration。\n",
    "\n",
    "\n",
    "- 然后，在 `repeat(3)` 返回的新 dataset 上调用 **`batch()`** 方法，这个方法也是返回一个新的 dataset。`batch(7)` 将前一个 dataset 的元素分组，7 个 items 为一个 batch。\n",
    "\n",
    "\n",
    "- 最后，在最终的 dataset 上进行 iteration。\n",
    "\n",
    "<br>\n",
    "\n",
    "`batch(7)` 输出的最后一个 batch 只有 2 个 items，而不是 7 个。可以使用 `drop_remainder=True` 来 drop 最后一个 batch，使得输出的所有 batch 大小相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.273506Z",
     "start_time": "2020-08-24T15:21:37.258263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Chain transformations\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "# 当最后一个 batch 的 size 小于 batch_size 的时候，drop 最后一个 batch\n",
    "dataset = dataset.repeat(3).batch(batch_size=7, drop_remainder=True)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b><font color='crimson'>dataset 的 method 不会 modify dataset，只会 return 一个新的 dataset，所以确保使用 <code>dataset = ...</code>，否则没有任何结果。</font></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.290174Z",
     "start_time": "2020-08-24T15:21:37.276027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Nothing will happen without a reference the new dataset\n",
    "dataset.repeat(3)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以调用 **`map()`** 方法来 transform dataset 中的 items。\n",
    "\n",
    "这个 method 可用来对 dataset 应用任何预处理手段。有时，应用的时包含的计算非常 intensive，比如 reshape 或者 rotate 一张图片，所以需要使用多线程 (multiple threads) 来加速，指定 **`num_parallel_calls`** 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.423592Z",
     "start_time": "2020-08-24T15:21:37.292546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Chain transformations\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.repeat(3).batch(batch_size=7)\n",
    "\n",
    "# 对 dataset 中的每一个元素应用 lambda 函数\n",
    "# 返回一个转换之后的元素组成的新 dataset\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.505554Z",
     "start_time": "2020-08-24T15:21:37.430409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Chain transformations\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.repeat(3).batch(batch_size=7)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    # A func mapping a dataset element to another dataset element\n",
    "    map_func=lambda x: x * 2,\n",
    "    # The number of parallel calls is set dynamically based on avaiable CPU\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.546338Z",
     "start_time": "2020-08-24T15:21:37.512231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  2,  4,  6,  8, 10, 12]),\n",
       " array([14, 16, 18,  0,  2,  4,  6]),\n",
       " array([ 8, 10, 12, 14, 16, 18,  0]),\n",
       " array([ 2,  4,  6,  8, 10, 12, 14]),\n",
       " array([16, 18])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将 dataset 中所有 item 转换为 ndarray\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`map()` 方法是对 dataset 中每一个 item 做转换，而 `apply()` 方法是对整个 dataset 做转换。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.595790Z",
     "start_time": "2020-08-24T15:21:37.551429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(10, shape=(), dtype=int64)\n",
      "tf.Tensor(12, shape=(), dtype=int64)\n",
      "tf.Tensor(14, shape=(), dtype=int64)\n",
      "tf.Tensor(16, shape=(), dtype=int64)\n",
      "tf.Tensor(18, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(10, shape=(), dtype=int64)\n",
      "tf.Tensor(12, shape=(), dtype=int64)\n",
      "tf.Tensor(14, shape=(), dtype=int64)\n",
      "tf.Tensor(16, shape=(), dtype=int64)\n",
      "tf.Tensor(18, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(10, shape=(), dtype=int64)\n",
      "tf.Tensor(12, shape=(), dtype=int64)\n",
      "tf.Tensor(14, shape=(), dtype=int64)\n",
      "tf.Tensor(16, shape=(), dtype=int64)\n",
      "tf.Tensor(18, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 对应 dataset 应用 unbatch 方法：\n",
    "# 将 dataset 中每个 item 转换为单整数 tensor，而不是 7 个整数组成的 batch\n",
    "dataset = dataset.unbatch()\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 **`filter()`** 方法来筛选 dataset："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.681798Z",
     "start_time": "2020-08-24T15:21:37.597812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset according to `predicate`\n",
    "dataset = dataset.filter(\n",
    "    # A function mapping a dataset element to a boolean.\n",
    "    predicate=lambda x: x < 10\n",
    ")\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.724202Z",
     "start_time": "2020-08-24T15:21:37.684431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 0, 2, 4, 6, 8, 0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 **`take()`** 方法来查看 dataset 中的 items："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.759181Z",
     "start_time": "2020-08-24T15:21:37.726751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Generate a dataset with at most `count` elements from this dataset\n",
    "for item in dataset.take(count=3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.791377Z",
     "start_time": "2020-08-24T15:21:37.761484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.take(3).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='crimson'>当训练集中的样本是独立同分布的 (independent and identically distributed, IID) 的时候，梯度下降的效果最好 (gradient descent works best) 。</font>\n",
    "\n",
    "一个简单的方法是使用 `shuffle()` 方法来打乱 (shuffle) 样本。\n",
    "\n",
    "**`shuffle()`** 返回一个新的 dataset：\n",
    "\n",
    "- 首先，使用 source dataset 的前部分数据 (the first items of the source dataset) 来填充 (fill up) 一个 buffer。\n",
    "\n",
    "\n",
    "- 然后，无论何时 ask for 一个 item 的时候，都会从 buffer 中随机选择一个 pull out，然后用 source dataset 中的下一个 item 来填充 buffer，直到 source dataset 里的数据用完了，这时会继续从 buffer 中随机 pull out 一个 item，直到 buffer 为空。\n",
    "\n",
    "**<font color='crimson'>必须指定 buffer 的大小 (size)，而且 buffer 应该足够大，否则 shuffle 没啥效果。不要比 RAM 大，而且即使 RAM 充足，也没必要超过 dataset 的大小。</font>**\n",
    "\n",
    "**<font color='crimson'>[TF docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle): For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.</font>**\n",
    "\n",
    "**<font color='blue'>例子：</font>** 数据集有 10000 个样本，buffer 的大小为 1000， buffer 初始时由数据集中的前 1000 个样本填充，即 `shuffle()` 第一次将随机从数据集的前 1000 个元素中随机选择一个。一旦一个元素被选中，它在 buffer 中位置由数据集中的下一个元素（即，第 1001 个元素）来替代，buffer 的大小维持在 1000，直至数据集用完了，这时继续从 buffer 中随机选择一个元素，直至 buffer 为空。\n",
    "\n",
    "<font color='blue'>每次运行程序时想要相同的随机顺序，应该指定 random seed。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.798217Z",
     "start_time": "2020-08-24T15:21:37.793878Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.820758Z",
     "start_time": "2020-08-24T15:21:37.801127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)  # 0 到 9，重复 3 次\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:37.846892Z",
     "start_time": "2020-08-24T15:21:37.828466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 6 5 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 2 1 0 4 6 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 2 5 9 2 1 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 3 8 7 9 5 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'>如果在一个 shuffle 后的 dataset 上调用 `repeat()` 方法，默认在每次 iteration 的时候都会产生一个新顺序 (new order)。</font>** 这通常来说都是不错的 idea，但是有时（如测试或 debugging）更希望在每次 iteration 后顺序保持不变。\n",
    "\n",
    "<br>\n",
    "\n",
    "对一个大 dataset（不能放进内存中）来说，这种简单的 shuffling-buffer 方法不够充分 (sufficient) ，因为相对于 dataset 来说，buffer 比较小。\n",
    "\n",
    "- **<font color='crimson'>一种解决方法是 shuffle 原始数据</font>**，如在 Linux 中使用 `shuf` 命令来 shuffle 文本文件。这种方法当然 (definitely) 可以大大提升 shuffling。即使原始数据已经 shuffle 了，通常你**还想进一步 shuffle，否则在每个 epoch 使用的数据顺序都相同，导致模型有偏差 (biased)**，如由于因为原始数据顺序的偶然性导致的虚假模式 (spurious patterns)。\n",
    "\n",
    "\n",
    "- **<font color='crimson'>为了进一步 shuffle 数据，一种通用的方法是将原始数据分成多个文件</font>**，然后在训练的时候随机读取。然而，同一个文件中的样本位置相近。为了避免这种情况，**<font color='crimson'>随机选择多个文件，同时从这些文件中读取数据，并交叉记录 (interleave records)。然后，再使用 `shuffle()` 方法</font>**。\n",
    "\n",
    "看起来很麻烦，实际使用 Data API 来处理很简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interleave lines from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 生成数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load、split 并且 scale California housing 数据，最后将 train/validation/test set 分别写进文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.285507Z",
     "start_time": "2020-08-24T15:21:37.852244Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    housing['data'], housing['target'].reshape(-1, 1), random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_mean = scaler.mean_\n",
    "x_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.293773Z",
     "start_time": "2020-08-24T15:21:38.288933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.89175860e+00,  2.86245478e+01,  5.45593655e+00,  1.09963474e+00,\n",
       "        1.42428122e+03,  2.95886657e+00,  3.56464315e+01, -1.19584363e+02])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.313113Z",
     "start_time": "2020-08-24T15:21:38.296196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.90927329e+00, 1.26409177e+01, 2.55038070e+00, 4.65460128e-01,\n",
       "       1.09576000e+03, 2.36138048e+00, 2.13456672e+00, 2.00093304e+00])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.325278Z",
     "start_time": "2020-08-24T15:21:38.314882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.442],\n",
       "       [1.687],\n",
       "       [1.621],\n",
       "       ...,\n",
       "       [0.68 ],\n",
       "       [0.613],\n",
       "       [1.97 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个不能读进内存中的大数据集，首先需要将数据集切分放到多个文件中，然后使用 TensorFlow 来并行 (in parallel) 读取这些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.339585Z",
     "start_time": "2020-08-24T15:21:38.328287Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_dataset_to_multiple_csv_files(data, name_prefix='train',\n",
    "                                       header=None, n_parts=10):\n",
    "    \"\"\"将数据分成若干份文件。\"\"\"\n",
    "    housing_dir = os.path.join('chap13_dataset', 'housing')\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, 'my_{}_{:02d}.csv')\n",
    "\n",
    "    filepaths = []\n",
    "    data_length = len(data)\n",
    "    # data 被分成 n_parts 份\n",
    "    data_parts_splited = np.array_split(np.arange(data_length), n_parts)\n",
    "    for file_idx, row_indices in enumerate(data_parts_splited):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "    \n",
    "        with open(part_csv, 'wt', encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write('\\n')\n",
    "            for row_idx in row_indices:\n",
    "                f.write(','.join([repr(col) for col in data[row_idx]]))\n",
    "                f.write('\\n')\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.347796Z",
     "start_time": "2020-08-24T15:21:38.341728Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = np.c_[x_train, y_train]\n",
    "val_data = np.c_[x_val, y_val]\n",
    "test_data = np.c_[x_test, y_test]\n",
    "\n",
    "header_cols = housing.feature_names + ['MedianHouseValue']\n",
    "header = ','.join(header_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.355958Z",
     "start_time": "2020-08-24T15:21:38.350498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.363172Z",
     "start_time": "2020-08-24T15:21:38.358539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'MedianHouseValue']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.593877Z",
     "start_time": "2020-08-24T15:21:38.365255Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据存在多个文件中\n",
    "train_filepaths = save_dataset_to_multiple_csv_files(\n",
    "    train_data, 'train', header, n_parts=20)\n",
    "val_filepaths = save_dataset_to_multiple_csv_files(\n",
    "    val_data, 'val', header, n_parts=20)\n",
    "test_filepaths = save_dataset_to_multiple_csv_files(\n",
    "    test_data, 'test', header, n_parts=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.607742Z",
     "start_time": "2020-08-24T15:21:38.596658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chap13_dataset/housing/my_train_00.csv',\n",
       " 'chap13_dataset/housing/my_train_01.csv',\n",
       " 'chap13_dataset/housing/my_train_02.csv',\n",
       " 'chap13_dataset/housing/my_train_03.csv',\n",
       " 'chap13_dataset/housing/my_train_04.csv',\n",
       " 'chap13_dataset/housing/my_train_05.csv',\n",
       " 'chap13_dataset/housing/my_train_06.csv',\n",
       " 'chap13_dataset/housing/my_train_07.csv',\n",
       " 'chap13_dataset/housing/my_train_08.csv',\n",
       " 'chap13_dataset/housing/my_train_09.csv',\n",
       " 'chap13_dataset/housing/my_train_10.csv',\n",
       " 'chap13_dataset/housing/my_train_11.csv',\n",
       " 'chap13_dataset/housing/my_train_12.csv',\n",
       " 'chap13_dataset/housing/my_train_13.csv',\n",
       " 'chap13_dataset/housing/my_train_14.csv',\n",
       " 'chap13_dataset/housing/my_train_15.csv',\n",
       " 'chap13_dataset/housing/my_train_16.csv',\n",
       " 'chap13_dataset/housing/my_train_17.csv',\n",
       " 'chap13_dataset/housing/my_train_18.csv',\n",
       " 'chap13_dataset/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.665132Z",
     "start_time": "2020-08-24T15:21:38.617704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.854307Z",
     "start_time": "2020-08-24T15:21:38.668273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\r\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\r\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\r\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\r\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 chap13_dataset/housing/my_train_00.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.867528Z",
     "start_time": "2020-08-24T15:21:38.859192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 构建一个输入 pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.899287Z",
     "start_time": "2020-08-24T15:21:38.870425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'chap13_dataset/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_19.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个只包含这些文件路径的 dataset\n",
    "filepath_dataset = tf.data.Dataset.from_tensor_slices(train_filepaths)\n",
    "for item in filepath_dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:38.930697Z",
     "start_time": "2020-08-24T15:21:38.901992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'chap13_dataset/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'chap13_dataset/housing/my_train_13.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "train_filepath_pattern = 'chap13_dataset/housing/my_train_*.csv'\n",
    "\n",
    "# A dataset of all files matching one or more glob patterns\n",
    "filepath_dataset = tf.data.Dataset.list_files(\n",
    "    file_pattern=train_filepath_pattern, seed=42)\n",
    "for item in filepath_dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法默认返回随机 shuffle 顺序的文件名，`shuffle=False` 或指定 `seed` 可以得到顺序确定 (deterministic order) 的结果。\n",
    "\n",
    "`train_filepath_pattern` 参数应该是少量的 (a small number of) glob patterns。如果文件名称已经 glob 了，使用 `tf.data.Dataset.from_tensor_slices(filenames)`，因为用 `list_files` re-globbing 每个文件名可能会导致远程存储系统 (remote storage system) 的 poor performance。\n",
    "\n",
    "<br>\n",
    "\n",
    "接下来，使用 **`interleave()`** 方法同时从 5 个文件中读取数据，并且 interleave 它们的行。使用 **`skip()`** 方法来跳过文件的第一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.014837Z",
     "start_time": "2020-08-24T15:21:38.933009Z"
    }
   },
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "interleave_dataset = filepath_dataset.interleave(\n",
    "    map_func=lambda file_path: tf.data.TextLineDataset(file_path).skip(count=1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`interleave()` 会创建一个新的 dataset —— `interleave_dataset`。\n",
    "\n",
    "**`interleave()`** 从 `filepath_dataset` 中选择 5 个文件路径，在每一个文件路径上调用 `map_func` 函数来创建一个新的 dataset （这里是 `TextLineDataset`）。准确地来说，在这个阶段有 7 个 dataset：文件路径 dataset（`filepath_dataset`）、interleave dataset （`interleave_dataset`）以及在 interleave dataset 内部创建的 5 个 `TextLineDataset`。\n",
    "\n",
    "**在 iterate `interleave_dataset` 的时候，将循环遍历 (cycle through) 这 5 个 `TextLineDataset`，每次从一个 `TextLineDataset` 中读取一行，直至所有的 `TextLineDataset` 为空。然后，从 `filepath_dataset` 中选择接下来的 5 个文件路径，以同样的方式进行 interleave，直到用完所有的文件路径。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>为了使 interleave 的效果最佳 (work best)，所有的文件长度应该相同 (identical length)。否则，最长文件的尾部数据不会被 interleave。</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T09:21:51.629509Z",
     "start_time": "2020-08-23T09:21:51.625921Z"
    }
   },
   "source": [
    "默认，`interleave()` 不会使用 parallelism。它每次只会顺序地从每个文件读取一行。\n",
    "\n",
    "将 `num_parallel_calls` 设置为你想的 threads，可以并行地 (in parallel) 读取文件。**`num_parallel_calls=tf.data.experiemntal.AUTOTUNE` 可以使 TensorFlow 根据可用的 (available) CPU 来动态地选择合适数量的 threads。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.064296Z",
     "start_time": "2020-08-24T15:21:39.016994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504', shape=(), dtype=string)\n",
      "tf.Tensor(b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for line in interleave_dataset.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上，是随机选择的 5 个 CSV 文件的第一行。\n",
    "\n",
    "这些数据需要进一步处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.075473Z",
     "start_time": "2020-08-24T15:21:39.067387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.89175860e+00,  2.86245478e+01,  5.45593655e+00,  1.09963474e+00,\n",
       "        1.42428122e+03,  2.95886657e+00,  3.56464315e+01, -1.19584363e+02])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.086150Z",
     "start_time": "2020-08-24T15:21:39.078794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.90927329e+00, 1.26409177e+01, 2.55038070e+00, 4.65460128e-01,\n",
       "       1.09576000e+03, 2.36138048e+00, 2.13456672e+00, 2.00093304e+00])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 **`tf.io.decode_csv()`** 可以将 CSV records 转换为 tensors。每一个 column 对应一个 \n",
    "tensor。\n",
    "\n",
    "该函数至少接收 2 个参数——第一个参数是需要解析的行，第二个参数是 CSV 文件每一个 column 的默认值组成的数组。这个默认值数组不仅指定了每个 column 的默认值，还指定了 column 的数量和它们的类型。\n",
    "\n",
    "**输出：** `tf.Tensor` 组成的 list。tensor 类型与 `record_defaults` 对应，每个 tensor 的 shape 相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.098727Z",
     "start_time": "2020-08-24T15:21:39.088433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_defaults = [0, np.nan, tf.constant(np.nan, dtype=tf.float64), 'hello',\n",
    "                  tf.constant([])]\n",
    "record_parsed = tf.io.decode_csv(\n",
    "    # string 类型的 Tensor\n",
    "    # 每个 string 是一个 row/record，所有的 records 应该具有相同的 format\n",
    "    records='1,2,3,4,5',\n",
    "    # 指定类型 tensor 组成的 list\n",
    "    record_defaults=record_defaults,\n",
    "    # recorde 的分隔符\n",
    "    field_delim=',')\n",
    "record_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.107635Z",
     "start_time": "2020-08-24T15:21:39.101143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column 为空的用 record_defaults 的对应值来替代\n",
    "record_parsed = tf.io.decode_csv(',,,,5', record_defaults)\n",
    "record_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第 5 个 column 是强制的 (compulsory)，因为指定的默认值是 `tf.constant([])`：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.116556Z",
     "start_time": "2020-08-24T15:21:39.109924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    record_parsed = tf.io.decode_csv(',,,,', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.130109Z",
     "start_time": "2020-08-24T15:21:39.120048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'hello'>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=1>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果最后一个 tf.constant(1)，就不会报错了\n",
    "record_defaults_2 = [0, np.nan, tf.constant(np.nan, dtype=tf.float64), 'hello',\n",
    "                     tf.constant(1)]\n",
    "tf.io.decode_csv(',,,,', record_defaults_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.138679Z",
     "start_time": "2020-08-24T15:21:39.133053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 fields but have 6 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# records 的长度应该与 record_defaults 的一致\n",
    "try:\n",
    "    tf.io.decode_csv('1,2,3,4,5,6', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.148941Z",
     "start_time": "2020-08-24T15:21:39.141302Z"
    }
   },
   "outputs": [],
   "source": [
    "def process(line, n_features=8, mean=x_mean, std=x_std):\n",
    "    record_defaults = [0.] * n_features + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=record_defaults)\n",
    "\n",
    "    # Stack tensors into a 1D array\n",
    "    x = tf.stack(\n",
    "        # shape 和 dtype 均相同的 tensor 组成的 list\n",
    "        values=fields[:-1],\n",
    "        axis=0)\n",
    "    # Generate a 1D array with a single value, rather than a scalar tensor\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - mean) / std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看代码吧：\n",
    "\n",
    "- 首先，假设每个特征的平均值 (mean) 和标准差 (standard deviation) 已经计算好了。`mean` 和 `std` 都是 包含 8 个浮点数的 1D tensor （或者 NumPy ndarray），每个特征对应一个值。\n",
    "\n",
    "\n",
    "- `process` 函数接收 CSV 中一行，然后 parse 它。为此，使用 `tf.io.decode_csv()` 函数，该函数接收 2 个参数——第一个参数是需要解析的行，第二个参数是 CSV 文件每一个 column 的默认值组成的数组。这个默认值数组不仅指定了每个 column 的默认值，还指定了 column 的数量和它们的类型。这里，所有的特征/column 都是 float，缺失值置为 0，最后一个 column 为 target，其默认值为 `tf.float32` 类型的空数组。这告诉 TensorFlow 这一个 column 包含 float，但是没有默认值，所以遇到缺失值的时候应该引发异常 (raise an exception)。\n",
    "\n",
    "\n",
    "- `tf.io.decode_csv` 函数返回一个由 scalar tensor （每个 column 一个）组成的 list。但是，需要的是 1D tensor 数组。所以，使用 `tf.stack()`\n",
    "\n",
    "\n",
    "- 最后，scale 特征，返回一个 features 和 target 组成 tuple。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.161561Z",
     "start_time": "2020-08-24T15:21:39.151516Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.172561Z",
     "start_time": "2020-08-24T15:21:39.163683Z"
    }
   },
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths,\n",
    "                       repeat=1,\n",
    "                       n_readers=5,\n",
    "                       n_read_threads=None,\n",
    "                       shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5,\n",
    "                       batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        map_func=lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=n_read_threads\n",
    "    )\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(\n",
    "        map_func=process,\n",
    "        num_parallel_calls=n_parse_threads\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>\n",
    "    \n",
    "![load and process data from multiple CSV files](chap13_figs/load_and_preprocess_data_from_multiple_CSV_file.png)\n",
    "<br>\n",
    "\n",
    "<center><i>load and process data from multiple CSV files</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.470926Z",
     "start_time": "2020-08-24T15:21:39.174974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
      "   1.2525111  -1.3671792 ]\n",
      " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
      "   0.7231292  -1.0023477 ]\n",
      " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
      "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[1.752]\n",
      " [1.313]\n",
      " [1.535]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
      "   0.9807942  -0.67250353]\n",
      " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
      "   1.107282   -0.8674123 ]\n",
      " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
      "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.919]\n",
      " [1.028]\n",
      " [2.182]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-1.0344558   1.0581076  -0.8869343  -0.08743899  0.6157541   0.4368748\n",
      "  -0.75726473  0.64688075]\n",
      " [ 0.4675818   0.6625668   0.03120424 -0.02621728 -0.8498953  -0.11217064\n",
      "  -0.860329    0.72684526]\n",
      " [-0.75702035  0.26702586 -0.60196173 -0.07209105  0.28082678  0.36254692\n",
      "  -0.7338394   0.7768212 ]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[1.364]\n",
      " [2.314]\n",
      " [1.696]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for x_batch, y_batch in train_set.take(3):\n",
    "    print(\"X =\", x_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `prefetch()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在最后调用 **`prefetch(1)`**，也会创建一个新的 dataset，这个 dataset 总是尽可能地提前一个 batch (do its best to always be one batch ahead)。即，在一个 batch 上训练的时候，dataset 会并行地准备下一个 batch（从 disk 中读取并处理）。\n",
    "\n",
    "<font color='blue'>这可以大大提升性能</font>。\n",
    "\n",
    "**如果能确保加载和预处理数据是线程的 (multithreaded)**（如，在 `interleave()` 和 `map()` 中设置 `num_parallel_calls`），**可以在 CPU 上利用多个核 (core)。如果准备一个 batch 的时间短于在一个 batch 在 GPU 上的训练时间（一个 training step），那么 GPU 的利用率几乎是 100%（不考虑数据从 CPU 到 GPU 的传输时间），训练将会非常快。**\n",
    "\n",
    "<br>\n",
    "\n",
    "![load and process data from multiple CSV files](chap13_figs/CPU_and_GPU_work_in_parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>如果打算买显卡，显卡的<font color='crimson'>处理能力 (processing power) </font>和<font color='crimson'>内存大小 (memory size) </font>是非常重要的。同样重要的还有<font color='crimson'>内存带宽 (memory bandwith) </font>——每秒从 RAM 读取多少 gigabytes 的数据或将多少 gigabytes 的数据存到 RAM 中。</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. `cache()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T14:40:26.391670Z",
     "start_time": "2020-08-24T14:40:26.386281Z"
    }
   },
   "source": [
    "如果**数据集可以 fit in memory**，那么调用 `cache()` 可以**大大地加速训练过程**。\n",
    "\n",
    "`cache()` 将数据内容 cache 进 RAM。\n",
    "\n",
    "**<font color='crimson'>通常在 loading 和 preprocessing 之后，但在 shuffling、batching 和 prefetching 之前调用。</font>** 这样，每个样本只会被读取和预处理一次，而不是每个 epoch 一次。但是，数据在每个 epoch 都会进行不同的 shuffle，而且，下一个 batch 会被提前准备。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 在 `tf.keras` 中使用 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:39.678880Z",
     "start_time": "2020-08-24T15:21:39.474811Z"
    }
   },
   "outputs": [],
   "source": [
    "# 不需要 repeat，tf.keras 会完成\n",
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "val_set = csv_reader_dataset(val_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:51.756849Z",
     "start_time": "2020-08-24T15:21:39.681263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 1s 4ms/step - loss: 2.0509 - val_loss: 4.6662\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.7463 - val_loss: 0.6669\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.6308 - val_loss: 0.6373\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5898 - val_loss: 0.5755\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5506 - val_loss: 0.5536\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5554 - val_loss: 0.5247\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5082 - val_loss: 0.5690\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5023 - val_loss: 0.4819\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4904 - val_loss: 0.4820\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4933 - val_loss: 0.4559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0a98976390>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_set,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(x_train) // 32,\n",
    "    validation_data=val_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:52.180708Z",
     "start_time": "2020-08-24T15:21:51.760032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46504756808280945"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:21:52.505664Z",
     "start_time": "2020-08-24T15:21:52.183208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.131139 ],\n",
       "       [1.9502337],\n",
       "       [1.3629727],\n",
       "       ...,\n",
       "       [1.9515147],\n",
       "       [5.962754 ],\n",
       "       [3.373827 ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不包含 labels，如果包含， keras 会忽略\n",
    "new_set = test_set.map(lambda x, y: x)\n",
    "\n",
    "model.predict(new_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要自定义 training loop，只要 iterate 训练集就好了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:22:01.463559Z",
     "start_time": "2020-08-24T15:21:52.508136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 1810/1810."
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_step_per_epoch = len(x_train) // batch_size\n",
    "total_steps = n_epochs * n_step_per_epoch\n",
    "global_step = 0\n",
    "\n",
    "for x_batch, y_batch in train_set.take(total_steps):\n",
    "    global_step += 1\n",
    "    print(\"\\rGlobal step {}/{}.\".format(global_step, total_steps), end=\"\")\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x_batch)\n",
    "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        loss = tf.add_n([main_loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "甚至，可以创建一个 TF 函数来完成整个 training step。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:22:01.479725Z",
     "start_time": "2020-08-24T15:22:01.466195Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32,\n",
    "          n_readers=5, n_read_threads=5,\n",
    "          shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(\n",
    "        train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
    "        n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
    "        n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
    "\n",
    "    for x_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:22:08.782972Z",
     "start_time": "2020-08-24T15:22:01.482324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T15:22:15.954994Z",
     "start_time": "2020-08-24T15:22:08.785263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 100 / 1810\n",
      "Global step 200 / 1810\n",
      "Global step 300 / 1810\n",
      "Global step 400 / 1810\n",
      "Global step 500 / 1810\n",
      "Global step 600 / 1810\n",
      "Global step 700 / 1810\n",
      "Global step 800 / 1810\n",
      "Global step 900 / 1810\n",
      "Global step 1000 / 1810\n",
      "Global step 1100 / 1810\n",
      "Global step 1200 / 1810\n",
      "Global step 1300 / 1810\n",
      "Global step 1400 / 1810\n",
      "Global step 1500 / 1810\n",
      "Global step 1600 / 1810\n",
      "Global step 1700 / 1810\n",
      "Global step 1800 / 1810\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32,\n",
    "          n_readers=5, n_read_threads=5,\n",
    "          shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(\n",
    "        train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
    "        n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
    "        n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
    "\n",
    "    n_steps_per_epoch = len(x_train) // batch_size\n",
    "    total_steps = n_epochs * n_steps_per_epoch\n",
    "    global_step = 0\n",
    "\n",
    "    for x_batch, y_batch in train_set:\n",
    "        global_step += 1\n",
    "        if tf.equal(global_step % 100, 0):\n",
    "            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这，学会使用 Data API 了。\n",
    "\n",
    "到目前为止，使用的都是 CSV 文件，虽然这很常用、简单和方便，但是效率并不高，而且不能很好地支持大的或者复杂的数据结构（如 images 或 audio）。这时就需要 TFRecords 了。\n",
    "\n",
    "当然，也不一定非要使用 TFRecords。当训练期间的主要 bottleneck 是加载和解析数据的时候，TFRecords 就很有用啦。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
